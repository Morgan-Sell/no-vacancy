name: CD - Model Validation & Promotion

on:
  workflow_dispatch:
    inputs:
      model_version:
        description: 'Model version to validate and promote'
        required: true
        type: string
      validation_notes:
        description: 'Validation notes from data scientist'
        required: false
        default: 'Manual validation completed'
        type: string
      auto_deploy:
        description: 'Automatically deploy after promotion to staging'
        required: false
        default: false
        type: boolean
      mock_validation:
        description: 'Use mock validation for testing'
        required: false
        default: true
        type: boolean
  push:
    branches:
      - deployment
    paths:
      - '.github/workflows/model_validation.yaml'

env:
  DB_USER: ${{ secrets.DB_USER }}
  DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
  DB_PORT: ${{ secrets.DB_PORT }}
  BRONZE_DB_HOST: ${{ secrets.BRONZE_DB_HOST }}
  BRONZE_DB: ${{ secrets.BRONZE_DB }}
  SILVER_DB_HOST: ${{ secrets.SILVER_DB_HOST }}
  SILVER_DB: ${{ secrets.SILVER_DB }}
  GOLD_DB_HOST: ${{ secrets.GOLD_DB_HOST }}
  GOLD_DB: ${{ secrets.GOLD_DB }}
  MLFLOW_DB_HOST: ${{ secrets.MLFLOW_DB_HOST }}
  MLFLOW_DB: ${{ secrets.MLFLOW_DB }}
  MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
  TEST_DB_HOST: ${{ secrets.TEST_DB_HOST }}
  TEST_DB: ${{ secrets.TEST_DB }}
  MLFLOW_EXPERIMENT_NAME: NoVacancyModelTraining
  # Test values for when triggered by push (not manual)
  TEST_MODEL_VERSION: "1"
  TEST_VALIDATION_NOTES: "Testing via push trigger"
  TEST_AUTO_DEPLOY: "false"
  TEST_MOCK_VALIDATION: "true"

jobs:
  validate-and-promote:
    name: Validate Model & Promote to Staging
    runs-on: ubuntu-latest
    outputs:
      promotion_successful: ${{ steps.promote.outcome }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Make setup_env.sh executable
        run: chmod +x app/scripts/setup_env.sh

      - name: Create .env file
        run: ./app/scripts/setup_env.sh

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set input values
        id: inputs
        run: |
          # Use manual inputs if available, otherwise use test defaults
          if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
            echo "MODEL_VERSION=${{ github.event.inputs.model_version }}" >> $GITHUB_OUTPUT
            echo "VALIDATION_NOTES=${{ github.event.inputs.validation_notes }}" >> $GITHUB_OUTPUT
            echo "AUTO_DEPLOY=${{ github.event.inputs.auto_deploy }}" >> $GITHUB_OUTPUT
            echo "MOCK_VALIDATION=${{ github.event.inputs.mock_validation }}" >> $GITHUB_OUTPUT
            echo "TRIGGER_TYPE=manual" >> $GITHUB_OUTPUT
          else
            echo "MODEL_VERSION=${{ env.TEST_MODEL_VERSION }}" >> $GITHUB_OUTPUT
            echo "VALIDATION_NOTES=${{ env.TEST_VALIDATION_NOTES }}" >> $GITHUB_OUTPUT
            echo "AUTO_DEPLOY=${{ env.TEST_AUTO_DEPLOY }}" >> $GITHUB_OUTPUT
            echo "MOCK_VALIDATION=${{ env.TEST_MOCK_VALIDATION }}" >> $GITHUB_OUTPUT
            echo "TRIGGER_TYPE=push" >> $GITHUB_OUTPUT
          fi

      - name: Show inputs
        run: |
          echo "üéØ Workflow Inputs:"
          echo "  Model Version: ${{ steps.inputs.outputs.MODEL_VERSION }}"
          echo "  Validation Notes: ${{ steps.inputs.outputs.VALIDATION_NOTES }}"
          echo "  Auto Deploy: ${{ steps.inputs.outputs.AUTO_DEPLOY }}"
          echo "  Mock Validation: ${{ steps.inputs.outputs.MOCK_VALIDATION }}"
          echo "  Trigger Type: ${{ steps.inputs.outputs.TRIGGER_TYPE }}"

      - name: Validate inputs
        run: |
          if [ -z "${{ steps.inputs.outputs.MODEL_VERSION }}" ]; then
            echo "‚ùå Model version cannot be empty"
            exit 1
          fi
          echo "‚úÖ All inputs are valid"

      - name: Validate model metrics
        id: validate
        run: |
          cd app
          python -c "
          import sys
          import os
          sys.path.append('.')

          model_version = '${{ steps.inputs.outputs.MODEL_VERSION }}'
          mock_validation = '${{ steps.inputs.outputs.MOCK_VALIDATION }}'.lower() == 'true'
          trigger_type = '${{ steps.inputs.outputs.TRIGGER_TYPE }}'

          print(f'üîç Validating model version: {model_version}')
          print(f'üß™ Mock validation mode: {mock_validation}')
          print(f'üöÄ Triggered by: {trigger_type}')

          if mock_validation:
              print('üß™ Running in MOCK mode for testing...')
              print(f'‚úÖ Model version {model_version} found (mocked)')
              print(f'üìä Test AUC: 0.92 (mocked)')
              print(f'üìä Validation AUC: 0.89 (mocked)')
              print(f'üéØ Minimum AUC threshold: 0.85')
              print(f'‚úÖ Model meets quality thresholds (mocked validation)')
          else:
              print('üîó Connecting to real MLflow...')
              try:
                  from services.mlflow_utils import MLflowArtifactLoader

                  loader = MLflowArtifactLoader()
                  model_details = loader.client.get_model_version(
                      name='${{ env.MLFLOW_EXPERIMENT_NAME }}',
                      version=model_version
                  )
                  print(f'‚úÖ Model version {model_version} found')

                  run = loader.client.get_run(model_details.run_id)
                  metrics = run.data.metrics

                  min_auc = 0.85
                  test_auc = metrics.get('test_auc', 0)
                  val_auc = metrics.get('val_auc', 0)

                  print(f'üìä Test AUC: {test_auc}')
                  print(f'üìä Validation AUC: {val_auc}')

                  if test_auc >= min_auc and val_auc >= min_auc:
                      print(f'‚úÖ Model meets quality thresholds (AUC >= {min_auc})')
                  else:
                      print(f'‚ùå Model does not meet quality thresholds')
                      sys.exit(1)

              except Exception as e:
                  print(f'‚ùå Error validating model: {e}')
                  print('üí° Set mock_validation=true to test without MLflow')
                  sys.exit(1)
          "

      - name: Promote model to staging
        id: promote
        if: steps.validate.outcome == 'success'
        run: |
          cd app
          python -c "
          import sys
          import os
          sys.path.append('.')

          model_version = '${{ steps.inputs.outputs.MODEL_VERSION }}'
          mock_validation = '${{ steps.inputs.outputs.MOCK_VALIDATION }}'.lower() == 'true'

          if mock_validation:
              print('üß™ MOCK: Simulating model promotion...')
              print(f'‚úÖ Model version {model_version} promoted to Staging (mocked)')
              print(f'üè∑Ô∏è  Added validation tags (mocked)')
          else:
              print('üîó Real MLflow promotion...')
              try:
                  from services.mlflow_utils import MLflowArtifactLoader
                  loader = MLflowArtifactLoader()

                  loader.client.transition_model_version_stage(
                      name='${{ env.MLFLOW_EXPERIMENT_NAME }}',
                      version=model_version,
                      stage='Staging',
                      archive_existing_versions=True
                  )

                  loader.client.set_model_version_tag(
                      name='${{ env.MLFLOW_EXPERIMENT_NAME }}',
                      version=model_version,
                      key='manual_validation',
                      value='approved'
                  )

                  print(f'‚úÖ Model version {model_version} promoted to Staging')

              except Exception as e:
                  print(f'‚ùå Error promoting model: {e}')
                  sys.exit(1)
          "

      - name: Create validation summary
        if: always()
        run: |
          echo "## üìä Model Validation Summary" >> $GITHUB_STEP_SUMMARY
          echo "- **Model Version:** ${{ steps.inputs.outputs.MODEL_VERSION }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Validated By:** ${{ github.actor }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Validation Status:** ${{ steps.promote.outcome }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Notes:** ${{ steps.inputs.outputs.VALIDATION_NOTES }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Trigger:** ${{ steps.inputs.outputs.TRIGGER_TYPE }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Mode:** ${{ steps.inputs.outputs.MOCK_VALIDATION == 'true' && 'Mock Testing' || 'Real MLflow' }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Timestamp:** $(date)" >> $GITHUB_STEP_SUMMARY

  auto-deploy:
    name: Auto-Deploy to Production
    needs: validate-and-promote
    runs-on: ubuntu-latest
    if: needs.validate-and-promote.outputs.promotion_successful == 'success'

    steps:
      - name: Check if auto-deploy enabled
        run: |
          # Get the auto_deploy setting from the previous job
          AUTO_DEPLOY="${{ needs.validate-and-promote.outputs.AUTO_DEPLOY || 'false' }}"

          if [ "$AUTO_DEPLOY" = "true" ]; then
            echo "üöÄ Auto-deploy is enabled"
          else
            echo "‚è∏Ô∏è  Auto-deploy is disabled - skipping deployment"
            exit 0
          fi

      - name: Simulate deployment
        run: |
          echo "üöÄ Would deploy to production here"
          echo "Model version: ${{ needs.validate-and-promote.outputs.MODEL_VERSION }}"
